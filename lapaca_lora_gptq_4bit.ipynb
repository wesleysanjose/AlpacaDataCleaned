{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesleysanjose/AlpacaDataCleaned/blob/main/lapaca_lora_gptq_4bit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the johnsmith0031 alpaca lora 4bit gptq implementation"
      ],
      "metadata": {
        "id": "-lVcjgziAze3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6Hli_Y2k_Bb"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/johnsmith0031/alpaca_lora_4bit\n",
        "%cd alpaca_lora_4bit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Found colab default py3.9 dosn't support well on installing the pip requirements. trying to download to use the py3.8 (but needing to install the pip for py3.8). also it looks like colab doesn't have pytorch installed."
      ],
      "metadata": {
        "id": "zS_vwc9bBvvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%ls /usr/bin/python*\n",
        "!python3.8 --version\n",
        "!sudo apt-get install python3-pip\n",
        "!python3.8 -m pip --version\n",
        "!python3.8 -m pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "xsmFedwAvsTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the default cuda11.8 doesn't work with this project, having to install cuda11.7"
      ],
      "metadata": {
        "id": "4IVNcFfgCRqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get remove \"*11-8*\" --allow-change-held-packages\n",
        "!sudo  apt install cuda-toolkit-11-7\n",
        "#!sudo apt list --installed | grep cuda"
      ],
      "metadata": {
        "id": "wwHZcEfx_nNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFqAPb_gqIWz"
      },
      "outputs": [],
      "source": [
        "!python3.8 -m pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "downloading the base model from HF"
      ],
      "metadata": {
        "id": "0tNpgYECCYxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/decapoda-research/llama-7b-hf\n",
        "!mv llama-7b-hf llama-13b-4bit"
      ],
      "metadata": {
        "id": "2LhX0mTB0nRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "downloading the 4bit gptq from HF"
      ],
      "metadata": {
        "id": "N11yxM8_CeGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/decapoda-research/llama-7b-hf-int4/resolve/main/llama-7b-4bit.pt\n",
        "!mv llama-7b-4bit.pt llama-13b-4bit.pt"
      ],
      "metadata": {
        "id": "as_e23-b23dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "downloading lora"
      ],
      "metadata": {
        "id": "ZyHJ2Pc6Ckp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/samwit/alpaca7B-lora"
      ],
      "metadata": {
        "id": "hKcY1Hb667sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "downloading stanford alpaca test dataset"
      ],
      "metadata": {
        "id": "zEXAQmKOCrpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/gururise/AlpacaDataCleaned/raw/main/alpaca_data_cleaned_archive.json"
      ],
      "metadata": {
        "id": "bzeBAcI08hU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.8 finetune.py alpaca_data_cleaned_archive.json \\\n",
        "    --ds_type=txt \\\n",
        "    --lora_out_dir=./test/ \\\n",
        "    --llama_q4_config_dir=./llama-13b-4bit/ \\\n",
        "    --llama_q4_model=./llama-13b-4bit.pt \\\n",
        "    --mbatch_size=1 \\\n",
        "    --batch_size=2 \\\n",
        "    --epochs=3 \\\n",
        "    --lr=3e-4 \\\n",
        "    --cutoff_len=256 \\\n",
        "    --lora_r=8 \\\n",
        "    --lora_alpha=16 \\\n",
        "    --lora_dropout=0.05 \\\n",
        "    --warmup_steps=5 \\\n",
        "    --save_steps=50 \\\n",
        "    --save_total_limit=3 \\\n",
        "    --logging_steps=5 \\\n",
        "    --groupsize=-1 \\\n",
        "    --v1 \\\n",
        "    --backend=cuda\n",
        "    #!python3.8 finetune.py alpaca_data_cleaned_archive.json"
      ],
      "metadata": {
        "id": "i4yqMvtTPP4q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4KHotGExXjDFfloFi6dwS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}